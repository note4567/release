""" 質疑応答 """

# colab 上での実行
# モデルのトレーニング(AutoModelForQuestionAnswering)
!git clone https://github.com/huggingface/transformers
%cd transformers
%pwd
!pip install .
!pip install fugashi==1.1.0 ipadic==1.0.0
%time
# 質問応答の学習
!python ./examples/legacy/question-answering/run_squad.py \
    --model_type=bert \
    --model_name_or_path=cl-tohoku/bert-base-japanese-whole-word-masking \
    --do_train \
    --do_eval \
    --train_file=/content/train_data/starbucks_train.json\
    --predict_file=/content/train_data/starbucks_dev.json \
    --per_gpu_train_batch_size 12 \
    --learning_rate 3e-5 \
    --num_train_epochs 5 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --overwrite_output_dir \
    --output_dir output/


from transformers import BertJapaneseTokenizer, AutoModelForQuestionAnswering
import torch
import json

# 入力テキスト
context = "港区は営業時間伸ばしています月～金：07:59～24:00土：01:00～18:00駐車場有:アクセスタクシー07:12／\n"
question="営業時間は?"

# モデルとトークナイザーの準備
%pwd
print("-"*32)
model = AutoModelForQuestionAnswering.from_pretrained('/content/transformers/output')
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')

# 推論の実行
inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"].tolist()[0]
output = model(**inputs)
answer_start = torch.argmax(output.start_logits)
answer_end = torch.argmax(output.end_logits) + 1
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

# 結果出力
print("質問: "+question)
print("応答: "+answer)