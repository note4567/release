""" 学習済みモデルを用いて AutoModelForQuestionAnswering を使う """
colab 上での実行
!pip install transformers fugashi==1.1.0 ipadic==1.0.0

from transformers import BertJapaneseTokenizer, AutoModelForQuestionAnswering
import torch
import json

# 入力テキスト
context = "港区は営業時間伸ばしています月～金：07:59～24:00土：01:00～18:00駐車場有:アクセスタクシー07:12／\n"
question="営業時間は?"

# モデルとトークナイザーの準備
%pwd
print("-"*32)
model = AutoModelForQuestionAnswering.from_pretrained('/content/transformers/output')
tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')

# 推論の実行
inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"].tolist()[0]
output = model(**inputs)
answer_start = torch.argmax(output.start_logits)
answer_end = torch.argmax(output.end_logits) + 1
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))

# 結果出力
print("質問: "+question)
print("応答: "+answer)

# テストデータを用いてテストする
with open("/content/train_data/starbucks_test.json") as f:
  # Jsonを読み込んで辞書として扱う
  test_j = json.load(f)
  for i in range(len(test_j["data"][0]["paragraphs"])):
    # 文章を抽出
    context = test_j["data"][0]["paragraphs"][i]["context"]
    print(f"[{i}]",context.strip())
    # 推論の実行
    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]
    output = model(**inputs)
    answer_start = torch.argmax(output.start_logits)
    answer_end = torch.argmax(output.end_logits) + 1
    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    # 結果出力
    print("質問: "+question)
    print("応答: "+answer)